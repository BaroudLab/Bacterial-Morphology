{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import ResNet34_Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_labeled_features(path,drop_non_labelled=True, potential_features=['rods','clumped','planktonic','filaments','positive','negative','intermediate']):\n",
    "    '''\n",
    "    we assume that the server is locally hosted as described in the settings, but the user can also\n",
    "    write sql code to read from the database directly\n",
    "    '''\n",
    "    response = requests.get(f\"http://localhost:8080/download_csv_by_path?path={path}\")\n",
    "    df = pd.read_csv(StringIO(response.text),index_col=0)\n",
    "    df.drop(df[df.trash==True].index,inplace=True) # drop all trash\n",
    "    \n",
    "    actual_feataures=list(df.columns)\n",
    "    features=[f for f in potential_features if f in actual_feataures]\n",
    "\n",
    "    # we add if its labelled\n",
    "    foo=pd.DataFrame(df.groupby(['chip','label']).apply(lambda x: x[features].any(axis=1),include_groups=False))\n",
    "    foo.reset_index(inplace=True)\n",
    "    foo=foo.drop(columns=['level_2'])\n",
    "    foo.rename(columns={0:'labeled'},inplace=True)\n",
    "    \n",
    "    df=df.merge(foo,on=['chip','label'])\n",
    "    if(drop_non_labelled):\n",
    "        df.drop(df[df.labeled==False].index,inplace=True) # drop all non labelled\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data_for_classification(path,columns_to_keep=['chip','label','concentration','positive','clumped','planktonic','filaments','rods','n_cells_1','n_cells_2','labeled','filename']):\n",
    "\n",
    "    data=load_labeled_features(path,drop_non_labelled=False)\n",
    "    \n",
    "    path_split=path.split('/')   # some datasets for Cipro are incosistently labelled and have a folder 1stexp and 2ndexp, we catch that here directly   \n",
    "    is_match = path_split[-2] in ['1stexp', '2ndexp', 'set-1', 'set-2', 'set1', 'set2']\n",
    "\n",
    "    if(is_match):\n",
    "        data['path_short']= path_split[-3] +'_'+ path_split[-2]\n",
    "    else:    \n",
    "        data['path_short']=path_split[-2]\n",
    "            \n",
    "    data['path']=path\n",
    "    data['filename']=''\n",
    "    prefix=data.path_short.unique()[0]\n",
    "    for c in data.chip.unique():\n",
    "        foo=data[data.chip==c].copy()\n",
    "        for i in foo.label.unique():\n",
    "            data.loc[(data.chip==c) & (data.label==i),'filename']=f'{prefix}_Crop_{c}_{i}.tiff'\n",
    "\n",
    "    # we need to convert 'negative' labeles into entries for positve if no column does not exist and add 0 entries if column does not exist\n",
    "    if ('negative' in data.columns) and ('positive' not in data.columns):\n",
    "        print('adding positive!')\n",
    "        data['positive']=1-data['negative'].values\n",
    "\n",
    "    for c in ['rods','filaments','planktonic','clumped']:\n",
    "            if(c not in data.columns):\n",
    "                data[c]=0.0\n",
    "    data.drop(columns=[c for c in data.columns if c not in columns_to_keep],inplace=True)\n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BacteriaEndPointDatasetApplication(Dataset): # here we use loc because we want to keept the index structure to insert later the labeles int othe orignal dataframe\n",
    "\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.info_table= df.copy()\n",
    "        self.ids= self.info_table.index.values\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        '''\n",
    "        loads image\n",
    "        '''\n",
    "        global_idx=self.ids[idx]\n",
    "        \n",
    "        img_name = os.path.join(self.root_dir,self.info_table.loc[global_idx].filename)\n",
    "        image=Image.open(img_name)\n",
    "        \n",
    "        if self.transform:\n",
    "            sample={'image': self.transform(image),'globel_id':global_idx}\n",
    "        else:\n",
    "            sample = {'image': image,'globel_id':global_idx}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Set up the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=5\n",
    "model = torchvision.models.resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "model.fc = torch.nn.Linear(512, num_classes) # affine linear transformation for last layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = 'trained_networks/' # where the trained network weights are stored\n",
    "modelname = 'bacteria_trained_model_resnet34'\n",
    "model.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, modelname), map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Load relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cipro\n",
    "Cipro_paths=['Cirpofloxacin/20230131-ecoli-cipro-1/day2',\n",
    "'Cirpofloxacin/20230131-ecoli-cipro-2/day2',\n",
    "'Cirpofloxacin/20220531-MIC-e.coli-cipro/2ndexp/day2',\n",
    "'Cirpofloxacin/20220531-MIC-e.coli-cipro/1stexp/day2']\n",
    "# Genta\n",
    "Genta_paths=['Gentamicin/20230110-e.coli-genta/day2', \n",
    "'Gentamicin/20230110-e.coli-genta-2/day2', \n",
    "'Gentamicin/20221101-ecoli-genta1/day2',\n",
    "'Gentamicin/20221101-ecoli-genta2/day2'] \n",
    "\n",
    "#Tetra\n",
    "Tetra_paths=['Tetracycline/20230404-ecoli-Tetracycline/set2/day2', \n",
    "'Tetracycline/20230404-ecoli-Tetracycline/set1/day2', \n",
    "'Tetracycline/20230315-ecoli/set-2/day2', \n",
    "'Tetracycline/20230315-ecoli/set-1/day2'] \n",
    "\n",
    "#CHP\n",
    "CHP_paths=['Chloramphenicol/20230313-ecoli-chp-2/day2',\n",
    "'Chloramphenicol/20230313-ecoli-chp-1/day2', \n",
    "'Chloramphenicol/20230221-ecoli-chp-1/day2', \n",
    "'Chloramphenicol/20230111-ecoli-chp-2/day2', \n",
    "'Chloramphenicol/20230111-ecoli-chp/day2', \n",
    "'Chloramphenicol/20221122-ecoli-chp/day2',\n",
    "'Chloramphenicol/20221031-ecoli-chp2/day2', \n",
    "'Chloramphenicol/20221031-ecoli-chp1/day2', \n",
    "'Chloramphenicol/20221013-ecoli-chp/day2', \n",
    "'Chloramphenicol/20221012-ecoli-chp/day2',\n",
    "'Chloramphenicol/20220628-MIC-e.coli-chp-LB-2/day2', \n",
    "'Chloramphenicol/20220628-MIC-e.coli-chp-LB-1/day2', \n",
    "'Chloramphenicol/20220602-MIC-e.coli-chp-LB/day2', \n",
    "'Chloramphenicol/20220524-MIC-e.coli-chp-LB/day2'] \n",
    "\n",
    "\n",
    "AMP_paths=['Ampicillin/20220614-MIC-e.coli-amp-LB-2/day2','Ampicillin/20220614-MIC-e.coli-amp-LB-1/day2']\n",
    "\n",
    "all_paths= Cipro_paths + Genta_paths + Tetra_paths + CHP_paths + AMP_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mounting_folder='/home/your_username/Crops/' # where the crops are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell will take a while to run: Docker slows the performance\n",
    "\n",
    "#predict label for not labelled data and fuse back into dataframe\n",
    "os.makedirs('PredictedLabelsTables', exist_ok=True) # creates folder to store the results if it does not exist\n",
    "for p in all_paths:\n",
    "    print(p)\n",
    "    data=prepare_data_for_classification(p)\n",
    "    data_to_process=data[data.labeled==False].copy() # only non labelled data\n",
    "    data_set_application=BacteriaEndPointDatasetApplication(data_to_process,mounting_folder,transforms.Compose([\n",
    "        transforms.Grayscale(3),\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))\n",
    "    dataloader_application=DataLoader(data_set_application,shuffle=False,batch_size=32,drop_last=False) # drops incomplete sets that don't match batch_size\n",
    "\n",
    "\n",
    "    pred_logit = []\n",
    "    id_list=[]\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,D in enumerate(dataloader_application):\n",
    "            images_to_clasify=D['image'].to(device)\n",
    "            pred_logit.append(model(images_to_clasify))\n",
    "            id_list.append(D['globel_id'].to(device))\n",
    "    \n",
    "    id_list=np.concatenate(id_list, axis=0)           \n",
    "    \n",
    "    pred_logit=np.concatenate(pred_logit, axis=0) \n",
    "    prob=1/(1+ np.exp(-pred_logit)) #sigmoid to get probability\n",
    "    threshold=0.5\n",
    "    y_pred=np.zeros_like(prob)\n",
    "    y_pred[prob>threshold]=1\n",
    "\n",
    "    for c,i in enumerate(id_list):\n",
    "        data.loc[i,'positive']=y_pred[c,0]\n",
    "        data.loc[i,'planktonic']=y_pred[c,1]\n",
    "        data.loc[i,'clumped']=y_pred[c,2]\n",
    "        data.loc[i,'rods']=y_pred[c,3]\n",
    "        data.loc[i,'filaments']=y_pred[c,4]\n",
    "     \n",
    "\n",
    "    data.to_csv(f'PredictedLabelsTables/{data.filename.unique()[0].split('_Crop')[0]}_trained.csv')\n",
    "\n",
    "    print(f'{data.filename.unique()[0].split('_Crop')[0]}_trained.csv done')\n",
    "    del data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
