{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import dask.array as da\n",
    "import skimage.io as io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_labeled_features(path,drop_non_labelled=True, potential_features=['rods','clumped','planktonic','filaments','positive','negative','intermediate']):\n",
    "    '''\n",
    "    we assume that the server is locally hosted as described in the settings, but the user can also\n",
    "    write sql code to read from the database directly\n",
    "    '''\n",
    "    response = requests.get(f\"http://localhost:8080/download_csv_by_path?path={path}\")\n",
    "    df = pd.read_csv(StringIO(response.text),index_col=0)\n",
    "    df.drop(df[df.trash==True].index,inplace=True) # drop all trash\n",
    "\n",
    "    df['dataset']=path.split('/')[-2]\n",
    "    \n",
    "    actual_feataures=list(df.columns)\n",
    "    features=[f for f in potential_features if f in actual_feataures]\n",
    "\n",
    "    # we add if its labelled\n",
    "    foo=pd.DataFrame(df.groupby(['chip','label']).apply(lambda x: x.loc[:,features].any(axis=1)))\n",
    "    foo.reset_index(inplace=True)\n",
    "    foo=foo.drop(columns=['level_2'])\n",
    "    foo.rename(columns={0:'labeled'},inplace=True)\n",
    "    \n",
    "    df=df.merge(foo,on=['chip','label'])\n",
    "    if(drop_non_labelled):\n",
    "        df.drop(df[df.labeled==False].index,inplace=True) # drop all non labelled\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    return df\n",
    "\n",
    "def to8bits(image, imin=None, imax=None): #min 400 max 450-600\n",
    "    \"\"\"\n",
    "    Converting to 8 bits\n",
    "    If min, max not provided, they are calculated automatically\n",
    "    \"\"\"\n",
    "    if imin is None:\n",
    "        imin = image.min()\n",
    "    if imax is None:\n",
    "        imax = image.max()\n",
    "        \n",
    "    # Clip the image to the specified range\n",
    "    image_clipped = np.clip(image, imin, imax)\n",
    "    \n",
    "    # Rescale the image to the range 0-255\n",
    "    image_8bit = ((image_clipped - imin) / (imax - imin) * 255).astype(np.uint8)\n",
    "    \n",
    "    return image_8bit    \n",
    "\n",
    "\n",
    "def bf_fluo_2rgb_black(fluo):\n",
    "    \"\"\"\n",
    "    creates rgb stack combining bf as grayscale and fluo as cyan\n",
    "    params:\n",
    "    -------\n",
    "    bf: 2D np.ndarray of type 'uint8'\n",
    "    fluo: 2D np.ndarray of type 'uint8'\n",
    "    \"\"\"\n",
    "    r = np.ones_like(fluo)\n",
    "    if fluo is not None:\n",
    "        g = fluo\n",
    "        \n",
    "       # g=fluo\n",
    "    else:\n",
    "        g = bf\n",
    "    b = r\n",
    "    return np.dstack((r, g, b)).astype(\"uint8\")\n",
    "\n",
    "def create_crops(df,mounting_folder,folder_out): \n",
    "    # for given path we will create crops of quadratic size for end point measurements: This assumes masked in aligend zarr file and hence crop size is fixed\n",
    "\n",
    "    prefix=df.path_short.unique()[0]\n",
    "    folder_in=df.path.unique()[0]\n",
    "    if not os.path.exists(folder_out):\n",
    "        os.makedirs(folder_out)\n",
    "\n",
    "    #load data\n",
    "    bf_tritc = da.from_zarr(f'{mounting_folder}{folder_in}/BF_TRITC_aligned.crops.zarr/') # droplet,chips, channel (bf,fluo), image coordinates\n",
    "    for c in df.chip.unique():# loop over chips \n",
    "        data=df[df.chip==c].copy()\n",
    "\n",
    "        for i in data.label.unique(): # loop over ids\n",
    "            tritc_image=bf_tritc[i-1,c,1,:].compute()\n",
    "            io.imsave(f'{folder_out}{prefix}_Crop_{c}_{i}.tiff',to8bits(image=tritc_image,imin=400,imax=600))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Create training data table and crops from database \n",
    "\n",
    "This script assumes that the webserver is running at  http://localhost:8080/ but one could also\n",
    "access the database directly.\n",
    "For faster training we create crops from the zarr files and save them directly. Also pytorch resnet requires 8 bit images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#these are all paths of the datasets provided \n",
    "\n",
    "# Cipro\n",
    "Cipro_paths=['Cirpofloxacin/20230131-ecoli-cipro-1/day2',\n",
    "'Cirpofloxacin/20230131-ecoli-cipro-2/day2',\n",
    "'Cirpofloxacin/20220531-MIC-e.coli-cipro/2ndexp/day2',\n",
    "'Cirpofloxacin/20220531-MIC-e.coli-cipro/1stexp/day2']\n",
    "# Genta\n",
    "Genta_paths=['Gentamicin/20230110-e.coli-genta/day2', \n",
    "'Gentamicin/20230110-e.coli-genta-2/day2', \n",
    "'Gentamicin/20221101-ecoli-genta1/day2',\n",
    "'Gentamicin/20221101-ecoli-genta2/day2'] \n",
    "\n",
    "#Tetra\n",
    "Tetra_paths=['Tetracycline/20230404-ecoli-Tetracycline/set2/day2', \n",
    "'Tetracycline/20230404-ecoli-Tetracycline/set1/day2', \n",
    "'Tetracycline/20230315-ecoli/set-2/day2', \n",
    "'Tetracycline/20230315-ecoli/set-1/day2'] \n",
    "\n",
    "#CHP\n",
    "CHP_paths=['Chloramphenicol/20230313-ecoli-chp-2/day2',\n",
    "'Chloramphenicol/20230313-ecoli-chp-1/day2', \n",
    "'Chloramphenicol/20230221-ecoli-chp-1/day2', \n",
    "'Chloramphenicol/20230111-ecoli-chp-2/day2', \n",
    "'Chloramphenicol/20230111-ecoli-chp/day2', \n",
    "'Chloramphenicol/20221122-ecoli-chp/day2',\n",
    "'Chloramphenicol/20221031-ecoli-chp2/day2', \n",
    "'Chloramphenicol/20221031-ecoli-chp1/day2', \n",
    "'Chloramphenicol/20221013-ecoli-chp/day2', \n",
    "'Chloramphenicol/20221012-ecoli-chp/day2',\n",
    "'Chloramphenicol/20220628-MIC-e.coli-chp-LB-2/day2', \n",
    "'Chloramphenicol/20220628-MIC-e.coli-chp-LB-1/day2', \n",
    "'Chloramphenicol/20220602-MIC-e.coli-chp-LB/day2', \n",
    "'Chloramphenicol/20220524-MIC-e.coli-chp-LB/day2'] \n",
    "\n",
    "\n",
    "AMP_paths=['Ampicillin/20220614-MIC-e.coli-amp-LB-2/day2','Ampicillin/20220614-MIC-e.coli-amp-LB-1/day2']\n",
    "\n",
    "all_paths= Cipro_paths + Genta_paths + Tetra_paths + CHP_paths + AMP_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now we collect all information for table and crop creation. If only training data is to be cropped\n",
    "#set drop_non_labelled=True\n",
    "# this cell will take a while to run:  Docker slows the performance\n",
    "df_list=[]\n",
    "for p in all_paths: \n",
    "    foo=load_labeled_features(p,drop_non_labelled=False) # load all data and drops thrash and then all non labelled if specified\n",
    "    # some datasets for Cipro and Tetra are incosistently labelled and have a folder 1stexp and 2ndexp, we catch that here directly\n",
    "    path_split=p.split('/')\n",
    "    is_match = path_split[-2] in ['1stexp', '2ndexp', 'set-1', 'set-2', 'set1', 'set2']\n",
    "    \n",
    "    if(is_match):\n",
    "        foo['path_short']= path_split[-3] +'_'+ path_split[-2]\n",
    "    else:    \n",
    "        foo['path_short']=path_split[-2]\n",
    "        \n",
    "    foo['path']=p\n",
    "    df_list.append(foo)\n",
    "\n",
    "df=pd.concat(df_list) \n",
    "df.reset_index(inplace=True)\n",
    "df = df.fillna(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.trash.unique(),df.labeled.unique()   # make sure all trash and not labelled are dropped (if wanted) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we set all intermediates to positive for this study\n",
    "df.loc[df.intermediate==1,'positive']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[(df.positive==True) & (df.negative==True)] #both true should be empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths_for_crops=df.path.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell will take a while to run\n",
    "\n",
    "\n",
    "folder_out='Crops/' #folder where Crops are stored: will be relative to the current path and created if not existing\n",
    "mounting_folder='/home/your_username/' #  where the image data is mounted \n",
    "for p in all_paths: \n",
    "    print(p)\n",
    "    create_crops(df[df.path==p],mounting_folder,folder_out)    \n",
    "    print(f'{p} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add the crop names into the table\n",
    "\n",
    "df['filename']=''\n",
    "for p in paths_for_crops:  \t\n",
    "    dg=df[df.path==p].copy()\n",
    "    prefix=dg.path_short.unique()[0]\n",
    "    for c in dg.chip.unique():\n",
    "        data=dg[dg.chip==c].copy()\n",
    "        for i in data.label.unique():\n",
    "            df.loc[(df.path==p) & (df.chip==c) & (df.label==i),'filename']=f'{prefix}_Crop_{c}_{i}.tiff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df.labeled==False].index,inplace=True) # drop not labelled if not allready dropped to save training information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save=df[['filename','positive','planktonic','clumped','rods','filaments']].copy()\n",
    "df_to_save[['positive','planktonic','clumped','rods','filaments']]=df_to_save[['positive','planktonic','clumped','rods','filaments']].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save.to_csv('../tables/LabelingSetAll.csv') # table with training information, we did not reindex after only selecting some data on purpose"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
